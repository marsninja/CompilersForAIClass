# ‚ö° Compilers and Runtimes for AI: From Prompts to Accelerators  
### EECS 598 ¬∑ Fall 2025  
*Cutting Edge and Emerging Technologies from the Programming Interface down to Hardware Acceleration of AI*

---

## üìñ Course Summary

The science and art of creating **efficient AI systems** spans the entire computing stack‚Äîfrom high-level language abstractions down to specialized hardware accelerators. This course provides a comprehensive exploration of AI compiler and runtime techniques, covering everything from **language-level AI compiler systems** (DSPy, SGLang, MTP, Guidance, LMQL) to **hardware-level acceleration** (GPU kernels, TPU compilation, custom ASICs, and emerging AI chips).

Students will learn how modern AI compilers and runtime systems like **PyTorch, JAX, TVM, TensorRT, VLLM, and specialized LLM compilers** orchestrate the full pipeline from prompt engineering and program synthesis down to optimized execution on heterogeneous hardware. The course covers the complete spectrum: **prompt-level optimizations**, **graph-level transformations**, **kernel-level tuning**, **memory hierarchy optimization**, and **distributed system coordination**.

### What You‚Äôll Do
- Design and implement **end-to-end optimized AI systems** spanning the full stack from language-level abstractions to hardware execution  
- Explore **language-level AI compiler techniques** (prompt optimization, program synthesis, declarative AI programming) and **traditional compiler optimizations** (graph-level transformations, kernel tuning, memory management, distributed training)  
- Work hands-on with **cutting-edge AI compiler ecosystems** (DSPy, SGLang, MTP, Guidance, LMQL, MLIR, TVM, PyTorch, CUDA) and **heterogeneous hardware platforms** (GPUs, TPUs, custom accelerators, emerging AI chips)  
- Present a **capstone project** demonstrating novel compiler/runtime optimizations for real-world AI workloads, with projects spanning the full spectrum from language-level innovations to hardware-level breakthroughs  

Projects are **team-based (3‚Äì5 students)** and include selecting a focus, designing the optimization approach, building compiler/runtime components, and benchmarking performance.  

### What You‚Äôll Learn
- The **comprehensive landscape of AI systems**: from language-level AI compiler techniques (DSPy, SGLang, MTP) to hardware-level acceleration (GPU kernels, TPU compilation, custom ASICs)  
- **State-of-the-art techniques**: prompt-level optimization, program synthesis, graph-level transformation, auto-tuning, quantization, inference acceleration, and emerging AI chip architectures  
- **Critical research skills**: interpreting papers, evaluating cutting-edge systems, presenting technical ideas, and bridging the gap between high-level AI programming and low-level hardware optimization  

Grading is **project-heavy**. You‚Äôll showcase your project‚Äôs evolution through presentations, paper reviews, and final demos.  

---

## üìÇ Tentative List of Topics
1. Introduction to AI Acceleration Systems  
2. Language-Level AI Compiler Systems (DSPy, SGLang, MTP, Guidance, LMQL)  
3. Prompt Engineering and Program Synthesis  
4. Deep Learning Computational Graphs  
5. Compiler Fundamentals for ML  
6. The PyTorch Ecosystem  
7. JAX and Functional Programming for ML  
8. TVM: Tensor Virtual Machine  
9. Quantization Techniques  
10. Memory Optimization  
11. GPU Programming for AI  
12. TensorRT and Inference Optimization  
13. Large Language Model Optimization  
14. Distributed Training Systems  
15. Heterogeneous Computing  
16. Hardware-Software Co-design  
17. Emerging AI Chip Architectures  
18. Emerging Techniques in AI Compilation  
19. Datacenter-scale AI Infrastructure  

---

## üë®‚Äçüè´ Instruction Team
- **Instructor**: [Jason Mars](http://www.jasonmars.org) (üìß profmars@umich.edu)  
- **GSI**: *TBD*  

---

## üóì Logistics
- **Lecture**: TBD  
- **Credits**: 4  
- **Office Hours**: On Demand  
- **GSI Office Hours**: TBA  
- **Course Discussion**: Piazza (TBD)  
- **Canvas**: TBD  
- **Recorded Lectures**: Available on Canvas  

---

## üìÖ Schedule
- **Weekly Schedule**: TBD *(subject to changes)*  

---

## üìä Grading
- TBD  

---

## üõ† Additional Details
- TBD  

---

‚≠ê *Prepare to build the next generation of compilers and runtimes for AI.*  
