# ‚ö° Compilers and Runtimes for AI: From Prompts to Accelerators  
### EECS 598 ¬∑ Fall 2025  
*Cutting Edge and Emerging Technologies from the Programming Interface down to Hardware Acceleration of AI*

---

## üìñ Course Summary

The science and art of creating **efficient AI systems** spans the entire computing stack‚Äîfrom high-level language abstractions down to specialized hardware accelerators. This course provides a comprehensive exploration of AI compiler and runtime techniques, covering everything from **language-level AI compiler systems** (DSPy, SGLang, MTP, Guidance, LMQL) to **hardware-level acceleration** (GPU kernels, TPU compilation, custom ASICs, and emerging AI chips).

Students will learn how modern AI compilers and runtime systems like **PyTorch, JAX, TVM, TensorRT, VLLM, and specialized LLM compilers** orchestrate the full pipeline from prompt engineering and program synthesis down to optimized execution on heterogeneous hardware. The course covers the complete spectrum: **prompt-level optimizations**, **graph-level transformations**, **kernel-level tuning**, **memory hierarchy optimization**, and **distributed system coordination**.

### What You‚Äôll Do
- Design and implement **end-to-end optimized AI systems** spanning the full stack from language-level abstractions to hardware execution  
- Explore **language-level AI compiler techniques** (prompt optimization, program synthesis, declarative AI programming) and **traditional compiler optimizations** (graph-level transformations, kernel tuning, memory management, distributed training)  
- Work hands-on with **cutting-edge AI compiler ecosystems** (DSPy, SGLang, MTP, Guidance, LMQL, MLIR, TVM, PyTorch, CUDA) and **heterogeneous hardware platforms** (GPUs, TPUs, custom accelerators, emerging AI chips)  
- Present a **capstone project** demonstrating novel compiler/runtime optimizations for real-world AI workloads, with projects spanning the full spectrum from language-level innovations to hardware-level breakthroughs  

Projects are **team-based (3‚Äì5 students)** and include selecting a focus, designing the optimization approach, building compiler/runtime components, and benchmarking performance.  

### What You‚Äôll Learn
- The **comprehensive landscape of AI systems**: from language-level AI compiler techniques (DSPy, SGLang, MTP) to hardware-level acceleration (GPU kernels, TPU compilation, custom ASICs)  
- **State-of-the-art techniques**: prompt-level optimization, program synthesis, graph-level transformation, auto-tuning, quantization, inference acceleration, and emerging AI chip architectures  
- **Critical research skills**: interpreting papers, evaluating cutting-edge systems, presenting technical ideas, and bridging the gap between high-level AI programming and low-level hardware optimization  

Grading is **project-heavy**. You‚Äôll showcase your project‚Äôs evolution through presentations, paper reviews, and final demos.  

---

## üìÇ Tentative List of Topics
1. Introduction to AI Acceleration Systems  
2. Language-Level AI Compiler Systems (DSPy, SGLang, MTP, Guidance, LMQL)  
3. Prompt Engineering and Program Synthesis  
4. Deep Learning Computational Graphs  
5. Compiler Fundamentals for ML  
6. The PyTorch Ecosystem  
7. JAX and Functional Programming for ML  
8. TVM: Tensor Virtual Machine  
9. Quantization Techniques  
10. Memory Optimization  
11. GPU Programming for AI  
12. TensorRT and Inference Optimization  
13. Large Language Model Optimization  
14. Distributed Training Systems  
15. Heterogeneous Computing  
16. Hardware-Software Co-design  
17. Emerging AI Chip Architectures  
18. Emerging Techniques in AI Compilation  
19. Datacenter-scale AI Infrastructure

## üìÇ Tentative List of Papers

| Paper | Link |
|------|-------|
| **DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines** | https://arxiv.org/pdf/2310.03714 |
| **Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs** | https://arxiv.org/abs/2406.11695 |
| **GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning** | https://arxiv.org/abs/2507.19457 |
| **SGLang: Efficient Execution of Structured Language Model Programs** | https://arxiv.org/abs/2312.07104 |
| **SGLang: Efficient Execution of Structured Language Model Programs** | https://arxiv.org/abs/2312.07104 |



---

## üë®‚Äçüè´ Instruction Team
- **Instructor**: [Jason Mars](http://www.jasonmars.org) (üìß profmars@umich.edu)  
- **GSI**: *TBD*  

---

## üóì Logistics
- **Lecture**: TBD  
- **Credits**: 4  
- **Office Hours**: On Demand  
- **GSI Office Hours**: TBA  
- **Course Discussion**: Piazza (TBD)  
- **Canvas**: TBD  
- **Recorded Lectures**: Available on Canvas  

---

## üìÖ Schedule

| Date | Topic | Description | Notes/Links |
|------|-------|-------------|-------------|
| **Aug 25, Mon** | Course Introduction & Overview | Lecture |  |
| **Aug 27, Wed** | Introduction to Compilers for AI | Lecture |  |
| **Sep 1, Mon** | Labor Day (Holiday) |  |  |
| **Sep 3, Wed** | Language-Level AI Compiler Systems I |  |  |
| **Sep 8, Mon** | Language-Level AI Compiler Systems II |  |  |
| **Sep 10, Wed** | Prompt Engineering and Program Synthesis |  |  |
| **Sep 15, Mon** | Deep Learning Computational Graphs |  |  |
| **Sep 17, Wed** | Compiler Fundamentals for ML |  |  |
| **Sep 22, Mon** | The PyTorch Ecosystem |  |  |
| **Sep 24, Wed** | JAX and Functional Programming for ML |  |  |
| **Sep 29, Mon** | TVM: Tensor Virtual Machine |  |  |
| **Oct 1, Wed** | Quantization Techniques |  |  |
| **Oct 6, Mon** | Memory Optimization |  |  |
| **Oct 8, Wed** | GPU Programming for AI |  |  |
| **Oct 13, Mon** | Fall Study Break (Holiday) |  |  |
| **Oct 15, Wed** | TensorRT and Inference Optimization |  |  |
| **Oct 20, Mon** | Large Language Model Optimization |  |  |
| **Oct 22, Wed** | Distributed Training Systems |  |  |
| **Oct 27, Mon** | Heterogeneous Computing |  |  |
| **Oct 29, Wed** | Hardware-Software Co-design |  |  |
| **Nov 3, Mon** | Emerging AI Chip Architectures |  |  |
| **Nov 5, Wed** | Emerging Techniques in AI Compilation |  |  |
| **Nov 10, Mon** | Datacenter-scale AI Infrastructure |  |  |
| **Nov 12, Wed** | Project Presentations I |  |  |
| **Nov 17, Mon** | Advanced Optimization Techniques |  |  |
| **Nov 19, Wed** | Guest Lectures |  |  |
| **Nov 24, Mon** | Project Work Session |  |  |
| **Nov 26, Wed** | Thanksgiving Recess (Holiday) |  |  |
| **Dec 1, Mon** | Final Project Presentations |  |  |
| **Dec 3, Wed** | Course Wrap-up & Future Directions |  |  |  

---

## üìä Grading
- TBD  

---

## üõ† Additional Details
- TBD  

---

‚≠ê *Prepare to build the next generation of compilers and runtimes for AI.*  
